{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49dbab34-3356-4fef-a6b3-d2d7291714cc",
   "metadata": {},
   "source": [
    "### Chihuahua Or Muffin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c18e0b-d328-4841-9957-b9d37cc284be",
   "metadata": {},
   "source": [
    "![Title](ressources/preview.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a093e3-3897-4351-8c94-00f52f42d841",
   "metadata": {},
   "source": [
    "#### Import des librairies de travail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7931203-83da-435b-abc0-b7b2fb5afd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# Représentation graphique\n",
    "import matplotlib.pyplot as plt\n",
    "# Commande pour afficher les plots inline\n",
    "%matplotlib inline\n",
    "# import our image opening tool\n",
    "from PIL import Image \n",
    "# import progress bars to show train progress\n",
    "from tqdm import tnrange, tqdm_notebook \n",
    "from glob import glob\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca77733b-0f91-47e1-a78d-71d4967f0f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch framework deep learning\n",
    "import torch\n",
    "# Extension Torchvision pour la gestion des datasets\n",
    "from torchvision import datasets, models, transforms\n",
    "# nn = Neural Network (définir les couches)\n",
    "import torch.nn as nn\n",
    "# Fonctions special\n",
    "from torch.nn import functional as F\n",
    "# Optimiseurs\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f8a46d-6e41-459e-bd8f-6176c620dd66",
   "metadata": {},
   "source": [
    "#### Construction du réseau de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69083e85-9a73-4808-ba16-d461f141dd21",
   "metadata": {},
   "source": [
    "Un réseau de neurone ressemble généralement à ce qui suit. Le nombre de neurones de sortie correspond au nombre de classes (ici 2 : chihuahua et muffin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62af334-c8b2-4ae8-9b87-23400980f3fa",
   "metadata": {},
   "source": [
    "![Title](ressources/what_is_nn_slide.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12048bc0-3e62-496a-bf5c-b1999659809b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySkynet(nn.Module):\n",
    "    \"\"\"\n",
    "    A very basic neural network.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(3, 224, 224)):\n",
    "        \"\"\"\n",
    "        Constructs a neural network.\n",
    "        \n",
    "        input_dim: a tuple that represents \"channel x height x width\" dimensions of the input\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # the total number of RGB pixels in an image is the tensor's volume\n",
    "        num_in_features = input_dim[0] * input_dim[1] * input_dim[2]\n",
    "        # input layer\n",
    "        self.layer_0 = nn.Linear(num_in_features, 128)\n",
    "        # hidden layers\n",
    "        self.layer_1 = nn.Linear(128, 64)\n",
    "        self.layer_2= nn.Linear(64, 32)\n",
    "        # output layer, output size of 2 for chihuahua and muffin\n",
    "        self.layer_3= nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define the forward pass through our network.\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        # convert our RGB tensor into one long vector\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        # pass through our layers\n",
    "        x = F.relu(self.layer_0(x))\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = F.relu(self.layer_3(x))\n",
    "        \n",
    "        # convert the raw output to probability predictions\n",
    "        x = F.softmax(x, dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8850c-b650-43bb-85fe-7ad83e799ce2",
   "metadata": {},
   "source": [
    "Le réseau de neurones étant défini, il faut l'initialiser. Si disponible, nous allons essayer d'utiliser le GPU; sinon le CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68f5e1b-f77f-46e8-accf-059958fc4a4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MySkynet(\n",
       "  (layer_0): Linear(in_features=150528, out_features=128, bias=True)\n",
       "  (layer_1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (layer_2): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (layer_3): Linear(in_features=32, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cuda:0 signifie le premier trouvé\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "model = MySkynet().to(device)                      \n",
    "# load our simple neural network\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4ee8c3-3e73-4d80-a3c3-d5c451a0ad6d",
   "metadata": {},
   "source": [
    "Voici à quoi ressemble notre réseau : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a856d19-8e40-469d-92f8-0fac3c18ffdc",
   "metadata": {},
   "source": [
    "![Title](ressources/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec1de1-c26f-422b-9140-42f5546349e2",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ab786-1292-4c8a-bb06-9acb9703fd1a",
   "metadata": {},
   "source": [
    "Il est nécessaire d'avoir deux jeux de données différents. Un pour l'entrainement de notre modèle; et un autre pour le tester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9278d77b-f9cb-4b46-8a93-7fbf1a971968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data contents: ['train', 'validation']\n",
      "Train contents: ['chihuahua', 'muffin']\n",
      "Validation contents: ['chihuahua', 'muffin']\n"
     ]
    }
   ],
   "source": [
    "print(\"Data contents:\", os.listdir(\"data\"))\n",
    "print(\"Train contents:\", os.listdir(\"data/train\"))\n",
    "print(\"Validation contents:\", os.listdir(\"data/validation\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566341f3-761c-4ede-b257-87054a18732a",
   "metadata": {},
   "source": [
    "Regardons quelques unes de nos images : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2f2cbd-ee3e-456a-b29b-7525d842405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 4, figsize=(15,60))  \n",
    "# to show 4 images side by side, make a \"1 row x 4 column\" axes\n",
    "ax[0].imshow(Image.open(\"data/train/chihuahua/4.jpg\"))\n",
    "ax[1].imshow(Image.open(\"data/train/chihuahua/5.jpg\"))\n",
    "ax[2].imshow(Image.open(\"data/train/muffin/131.jpg\"))\n",
    "ax[3].imshow(Image.open(\"data/train/muffin/107.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705313a4-4028-44cb-9d75-6b722039d923",
   "metadata": {},
   "source": [
    "Notre séparation jeux d'entrainement et de test est en 80/20. 120 images pour l'entrainement et 30 images pour la validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1421bb2a-7fda-4c10-80ce-3130864cf9a6",
   "metadata": {},
   "source": [
    "![Title](ressources/folders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a07592-6348-4224-8c23-24149d64ad55",
   "metadata": {},
   "source": [
    "##### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af52065a-8249-49af-9db4-a88eac12fff8",
   "metadata": {},
   "source": [
    "Il faut maintenant charger nos images et les convertir pour les rendre compréhensibles pour notre réseau de neurones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0688e0b-8e86-47c0-af64-88863084a7bf",
   "metadata": {},
   "source": [
    "PyTorch utilise des objets Tensor (matrices multidimensionnelles - Nd arrays)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5695b2-f776-4638-85ea-877abb85c99a",
   "metadata": {},
   "source": [
    "Pour convertir les images en tensors, on va passer par un \"Dataloader\". Le dataloader organise nos données pour que le modèle puisse les utiliser. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b2f35e-79cb-4028-b72c-393a76fa4271",
   "metadata": {},
   "source": [
    "![Title](ressources/image_to_tensor.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434d1689-b4b6-4e6b-a6c8-a7ce9f404fac",
   "metadata": {},
   "source": [
    "Il faut tout d'abord définir des \"transformations\" pour convertir nos images en tensors. Et cela, pour nos deux datasets (train/test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5bd0c33-8bd0-4d4e-8226-c8cff9384de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train transforms: Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                                 std=[0.5, 0.5, 0.5])\n",
    "\n",
    "# transforms for our training data\n",
    "train_transforms = transforms.Compose([\n",
    "    # resize to resnet input size\n",
    "    transforms.Resize((224,224)),\n",
    "    # transform image to PyTorch tensor object\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "# these validation transforms are exactly the same as our train transforms\n",
    "validation_transforms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "print(\"Train transforms:\", train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d361c-d166-4436-b80c-e00dc6d123b1",
   "metadata": {},
   "source": [
    "Ensuite, il faut créer les datasets en passing les transformations dans le constructeur ImageFolder()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70957d50-3eef-4039-9d2e-27474486e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==Train Dataset==\n",
      " Dataset ImageFolder\n",
      "    Number of datapoints: 120\n",
      "    Root location: data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n",
      "\n",
      "==Validation Dataset==\n",
      " Dataset ImageFolder\n",
      "    Number of datapoints: 120\n",
      "    Root location: data/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "image_datasets = {\n",
    "    'train':\n",
    "        datasets.ImageFolder('data/train', train_transforms),\n",
    "    'validation':\n",
    "        datasets.ImageFolder('data/validation', validation_transforms)}\n",
    "\n",
    "print(\"==Train Dataset==\\n\", image_datasets[\"train\"])\n",
    "print()\n",
    "print(\"==Validation Dataset==\\n\", image_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95765c4a-c262-4434-ab82-37e7df4d2976",
   "metadata": {},
   "source": [
    "Enfin, créer les dataloaders à partir des datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5c289fd-c45f-47f4-97cc-6d611cae199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader: <torch.utils.data.dataloader.DataLoader object at 0x0000017261CC8280>\n",
      "Validation loader: <torch.utils.data.dataloader.DataLoader object at 0x0000017261CC8310>\n"
     ]
    }
   ],
   "source": [
    "dataloaders = {\n",
    "    'train':\n",
    "        torch.utils.data.DataLoader(\n",
    "            image_datasets['train'],\n",
    "            batch_size=8,\n",
    "            shuffle=True,\n",
    "            num_workers=4),\n",
    "    'validation':\n",
    "        torch.utils.data.DataLoader(\n",
    "            image_datasets['validation'],\n",
    "            batch_size=8,\n",
    "            shuffle=False,\n",
    "            num_workers=4)}\n",
    "\n",
    "print(\"Train loader:\", dataloaders[\"train\"])\n",
    "print(\"Validation loader:\", dataloaders[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbf7b80-ba88-4d84-8ad7-874aa2a9f08f",
   "metadata": {},
   "source": [
    "Un dataloader renvoie deux choses : un tensor pour représenter une image et un vecteur pour représenter les labels (0 ou 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dc99f7-ecdf-4687-91e8-bae9cd20c3cd",
   "metadata": {},
   "source": [
    "![Title](ressources/training_testing_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113e648a-a679-43b4-a376-03324429b78f",
   "metadata": {},
   "source": [
    "#### Entrainer le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ca2833-3b8e-42fa-88c5-75be94bfce36",
   "metadata": {},
   "source": [
    "Nous avons le réseau de neurones et les données. Il faut maintenant entrainer le modèle en itérant sur les données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8fb9ca-104a-447a-a8dc-04e02f2e14c2",
   "metadata": {},
   "source": [
    "![Title](ressources/backpropagation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590a2e5e-00a6-4067-b90c-0e5f7749a26a",
   "metadata": {},
   "source": [
    "Pour obtenir un meilleur résultat il faut que notre réseau de neurones puissent s'entrainer plusieurs fois. On va donc passer nos données via de multiples epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8be2d6-27ce-456b-b772-abd629aa12c3",
   "metadata": {},
   "source": [
    "Après chaque epoch, on vérifie comment notre modèle performe sur le jeu de validation pour suivre ses progrès."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f79fabf4-fe9b-4b44-ac5d-ef7172bdbea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, loss_function, optimizer, num_epochs):\n",
    "    \"\"\"\n",
    "    Trains a model using the given loss function and optimizer, for a certain number of epochs.\n",
    "    \n",
    "    model: a PyTorch neural network\n",
    "    loss_function: a mathematical function that compares predictions and labels to return an error\n",
    "    num_epochs: the number of times to run through the full training dataset\n",
    "    \"\"\"\n",
    "    # train for n epochs. an epoch is a full iteration through our dataset\n",
    "    for epoch in tnrange(num_epochs, desc=\"Total progress\", unit=\"epoch\"):\n",
    "        # print a header\n",
    "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
    "        print('----------------')\n",
    "\n",
    "        # first train over the dataset and update weights; at the end, calculate our validation performance\n",
    "        for phase in ['train', 'validation']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval() \n",
    "\n",
    "            # keep track of the overall loss and accuracy for this batch\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "             # iterate through the inputs and labels in our dataloader\n",
    "            # (the tqdm_notebook part is to display a progress bar)\n",
    "            for inputs, labels in tqdm_notebook(dataloaders[phase], desc=phase, unit=\"batch\", leave=False):\n",
    "                # move inputs and labels to appropriate device (GPU or CPU)\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # FORWARD PASS\n",
    "                outputs = model(inputs)\n",
    "                # compute the error of the model's predictions\n",
    "                loss = loss_function(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    # BACKWARD PASS\n",
    "                    optimizer.zero_grad()  # clear the previous gradients\n",
    "                    loss.backward()        # backpropagate the current error gradients\n",
    "                    optimizer.step()       # update the weights (i.e. do the learning)\n",
    "\n",
    "                # track our accumulated loss\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                # track number of correct to compute accuracy\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "                # print our progress\n",
    "            epoch_loss = running_loss / len(image_datasets[phase])\n",
    "            epoch_acc = running_corrects.double() / len(image_datasets[phase])\n",
    "            print(f'{phase} error: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b7825-bd20-4670-9024-98d97bfc8db1",
   "metadata": {},
   "source": [
    "Une dernière chose à faire : définir une fonction qui nous renseigne sur la performance de notre modèle. C'est la fonction de perte (loss function) qui compare les prédictions du modèle aux vrais labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9ba81b-0f34-4f79-af40-5b09b9fa85bb",
   "metadata": {},
   "source": [
    "Une fois la perte (l'erreur) calculée, on doit définir comment notre modèle doit réagir à ce feedback. Un optimiseur détermine comment le réseau de neurones apprend du feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e1c18b5-c6bb-4420-8d4a-7bf4f928cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss()              \n",
    "# the most common error function in deep learning\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)  \n",
    "# Stochastic Gradient Descent, with a learning rate of 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08aece1-c6a1-4674-aef2-893bbeb9f3f6",
   "metadata": {},
   "source": [
    "![Title](ressources/gradient_descent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01ba19a-1fd5-489a-ac52-d05210e00ee8",
   "metadata": {},
   "source": [
    "Action !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "426474d6-0140-4fb3-8be2-bcdc5ecbd99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-0f2f39ac3159>:10: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for epoch in tnrange(num_epochs, desc=\"Total progress\", unit=\"epoch\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c8cdf5c28a42a6b0839f3d00698554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Total progress:   0%|          | 0/3 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-0f2f39ac3159>:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for inputs, labels in tqdm_notebook(dataloaders[phase], desc=phase, unit=\"batch\", leave=False):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/15 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.6492, Accuracy: 0.6417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error: 0.5503, Accuracy: 0.8000\n",
      "\n",
      "Epoch 2/3\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/15 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.5102, Accuracy: 0.8417\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error: 0.5615, Accuracy: 0.7000\n",
      "\n",
      "Epoch 3/3\n",
      "----------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train:   0%|          | 0/15 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error: 0.3943, Accuracy: 0.9333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation:   0%|          | 0/4 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation error: 0.4584, Accuracy: 0.8667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_model(model, dataloaders, loss_function, optimizer, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252d574-94ef-4015-b517-419e5942a147",
   "metadata": {},
   "source": [
    "On observe désormais l'accuracy qui correspond au score de notre modèle (fréquence de prédiction correcte)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c54b2-1a6b-4e84-aef7-7490129d9be3",
   "metadata": {},
   "source": [
    "On peut visualiser les prédictions de notre modèle comme suit : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c26558-a1d9-43fc-8f1f-373674fe588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the images from our validation sets\n",
    "validation_img_paths = glob(\"data/validation/**/*.jpg\", recursive=True)\n",
    "images = [Image.open(img_path) for img_path in validation_img_paths]\n",
    "\n",
    "# put all the images together to run through our model\n",
    "validation_batch = torch.stack( [validation_transforms(img).to(device) for img in images])\n",
    "pred_logits_tensor = model(validation_batch)\n",
    "pred_probs = pred_logits_tensor.cpu().data.numpy()\n",
    "\n",
    "# show the probabilities for each picture\n",
    "fig, axs = plt.subplots(6, 5, figsize=(20, 20))\n",
    "for i, img in enumerate(images):\n",
    "    ax = axs[floor(i/5)][i % 5]\n",
    "    ax.axis('off')\n",
    "    ax.set_title(\"{:.0f}% Chi, {:.0f}% Muff\".format(100*pred_probs[i,0], 100*pred_probs[i,1]), fontsize=18)\n",
    "    ax.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
